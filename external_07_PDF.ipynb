{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANG_SMITH\")\n",
    "os.environ[\"ANTHROPIC_API_KEY\"] = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"TAVILY_API_KEY\"] = os.getenv(\"TAVILY_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF 파일 로드\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "file_path = r\"c:\\Users\\kmidd\\Desktop\\2406.15057v1.pdf\"\n",
    "loader = PyPDFLoader(file_path)\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 지식 베이스 생성\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":6})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'What is latent space?',\n",
       " 'context': [Document(metadata={'page': 11, 'source': 'c:\\\\Users\\\\kmidd\\\\Desktop\\\\2406.15057v1.pdf'}, page_content='Latent Space Translation via Inverse Relative Projection\\n2020, Addis Ababa, Ethiopia, April 26-30, 2020 . OpenRe-\\nview.net, 2020. URL https://openreview.net/\\nforum?id=HyebplHYwB .\\nVedula, S., Maiorca, V ., Basile, L., Locatello, F., and Bron-\\nstein, A. Scalable unsupervised alignment of general\\nmetric and non-metric structures, 2024.\\nVuli´c, I., Ruder, S., and Søgaard, A. Are all good word\\nvector spaces isomorphic? In Proceedings of the\\n2020 Conference on Empirical Methods in Natural Lan-\\nguage Processing (EMNLP) , pp. 3178–3192, Online,\\n2020. Association for Computational Linguistics. doi:\\n10.18653/v1/2020.emnlp-main.257. URL https://\\naclanthology.org/2020.emnlp-main.257 .\\nWang, Y ., Liu, Y ., and Ma, Z.-M. The scale-invariant space\\nfor attention layer in neural network. Neurocomputing ,\\n392:1–10, 2020.\\nWang, Z., Shan, X., Zhang, X., and Yang, J. N24news:\\nA new dataset for multimodal news classification. In\\nProceedings of the Language Resources and Evaluation'),\n",
       "  Document(metadata={'page': 10, 'source': 'c:\\\\Users\\\\kmidd\\\\Desktop\\\\2406.15057v1.pdf'}, page_content='Latent Space Translation via Inverse Relative Projection\\nLenc, K. and Vedaldi, A. Understanding image repre-\\nsentations by measuring their equivariance and equiv-\\nalence. In IEEE Conference on Computer Vision and\\nPattern Recognition, CVPR 2015, Boston, MA, USA,\\nJune 7-12, 2015 , pp. 991–999. IEEE Computer Society,\\n2015. doi: 10.1109/CVPR.2015.7298701. URL https:\\n//doi.org/10.1109/CVPR.2015.7298701 .\\nLi, H., Xu, Z., Taylor, G., Studer, C., and Goldstein,\\nT. Visualizing the loss landscape of neural nets. In\\nBengio, S., Wallach, H. M., Larochelle, H., Grauman,\\nK., Cesa-Bianchi, N., and Garnett, R. (eds.), Advances in\\nNeural Information Processing Systems 31: Annual Con-\\nference on Neural Information Processing Systems 2018,\\nNeurIPS 2018, December 3-8, 2018, Montr ´eal, Canada ,\\npp. 6391–6401, 2018. URL https://proceedings.\\nneurips.cc/paper/2018/hash/\\na41b3bb3e6b050b6c9067c67f663b915-Abstract.\\nhtml .\\nLi, Y ., Yosinski, J., Clune, J., Lipson, H., and Hopcroft,'),\n",
       "  Document(metadata={'page': 0, 'source': 'c:\\\\Users\\\\kmidd\\\\Desktop\\\\2406.15057v1.pdf'}, page_content='Latent Space Translation via Inverse Relative Projection\\nValentino Maiorca* 1 2Luca Moschella* 1 2Marco Fumero1 2Francesco Locatello2Emanuele Rodol `a1\\nAbstract\\nThe emergence of similar representations between\\nindependently trained neural models has sparked\\nsignificant interest in the representation learning\\ncommunity, leading to the development of vari-\\nous methods to obtain communication between\\nlatent spaces. Latent space communication can be\\nachieved in two ways: i) by independently map-\\nping the original spaces to a shared or relative one\\n(Moschella et al., 2023); ii) by directly estimating\\na transformation from a source latent space to a\\ntarget one (Maiorca et al., 2024). In this work,\\nwe combine the two into a novel method to ob-\\ntain latent space translation through the relative\\nspace. By formalizing the invertibility of angle-\\npreserving relative representations and assuming\\nthe scale invariance of decoder modules in neural\\nmodels, we can effectively use the relative space'),\n",
       "  Document(metadata={'page': 2, 'source': 'c:\\\\Users\\\\kmidd\\\\Desktop\\\\2406.15057v1.pdf'}, page_content='ing that could mitigate the impact of not centered spaces.\\n3.2. Latent Space Translation\\nThe key benefit of the relative projection is its non-injective\\nnature, as it maps different absolute spaces into a single\\nrelative space. The core of our method lies in the formal-\\nization of the inverse process by exploiting the information\\ncontained in the anchor points. At a high level, this means\\nwe can use the relative space as a middle ground to translate\\nan encoding from an absolute space Xto any other seman-\\ntically similar absolute space Y. This can be formalized\\nas:\\nYabs=Xrel·(AT\\nY)−1, (2)\\nunder the assumption that Xrel≈Yrel. This transforma-\\ntion of relative to absolute coordinates allows us to transfer\\nany encoding between absolute spaces using only a subset\\nof points from both spaces (the anchors). The anchors of\\nthe source space, the encoding space X, are used to project\\ninto the shared relative space, while the anchors of the target'),\n",
       "  Document(metadata={'page': 1, 'source': 'c:\\\\Users\\\\kmidd\\\\Desktop\\\\2406.15057v1.pdf'}, page_content='(Moschella et al., 2023), enabling latent spaces of arbitrary\\nneural models to ”communicate”. The method introduces an\\nalternative way of representing samples in the latent spaces\\nof neural networks by shifting the perspective from an abso-\\nlute coordinate system to one relative to a set of predefined'),\n",
       "  Document(metadata={'page': 2, 'source': 'c:\\\\Users\\\\kmidd\\\\Desktop\\\\2406.15057v1.pdf'}, page_content='Furthermore, they demonstrate that different latent spaces,\\nin practice, differ only by an isometry transformation plus\\nlocal rescaling if they share the same data semantics. This is\\nalso shown in (Maiorca et al., 2024) where Procrustes anal-\\nysis is efficiently used to map with high performance from\\nspaceXtoY, The discovery that a rigid transformation is\\nenough to map between the two is crucial for our work as it\\nallows us to estimate the translation, rescaling, rotation, and\\nreflection constituting this isometry separately, enabling the\\ndecoding of a single relative space into different absolute\\nspaces . While the original relative representations method\\nassumes that ”NNs commonly employ normalization tech-\\nniques...to center the latent spaces around zero”, we add a\\ncentering step to enforce it since we do not rely on any train-\\ning that could mitigate the impact of not centered spaces.\\n3.2. Latent Space Translation\\nThe key benefit of the relative projection is its non-injective')],\n",
       " 'answer': \"A latent space refers to the internal representations or encodings learned by a neural network within its hidden layers. Specifically:\\n\\n1) Neural networks learn to map input data (like images or text) into a latent or encoding space, which captures salient features of the input in a lower-dimensional representation.\\n\\n2) This latent representation exists in the network's hidden layers before being mapped to the final output.\\n\\n3) The latent space representations are learned in a way that helps the network perform the desired task well (e.g. classification, generation, etc.).\\n\\nSo in summary, the latent space refers to the compressed feature representations that a neural network learns as an intermediate step before producing its final outputs. These latent representations aim to capture the most informative factors of variation in the input data.\"}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "llm = ChatAnthropic(model=\"claude-3-sonnet-20240229\")\n",
    "\n",
    "# RAG chain 만들기\n",
    "\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "\n",
    "results = rag_chain.invoke({\"input\":\"What is latent space?\"})\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
