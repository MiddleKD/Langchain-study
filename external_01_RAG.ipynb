{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANG_SMITH\")\n",
    "os.environ[\"ANTHROPIC_API_KEY\"] = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"TAVILY_API_KEY\"] = os.getenv(\"TAVILY_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "llm = ChatAnthropic(model=\"claude-3-sonnet-20240229\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 알아서 크롤링\n",
    "\n",
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "bs4_strainer = bs4.SoupStrainer(class_=(\"post-title\", \"post-header\", \"post-content\"))\n",
    "loader = WebBaseLoader(\n",
    "    web_path=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs={\"parse_only\": bs4_strainer}\n",
    ")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# document 청크하기\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=200, add_start_index = True\n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 지식베이스 저장소 생성\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=all_splits, \n",
    "                                    embedding=OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":6})\n",
    "retriever_docs = retriever.invoke(\"What are the approaches to Task Decomposition?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\n",
      "Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\n"
     ]
    }
   ],
   "source": [
    "print(retriever_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: filler question \\nContext: filler context \\nAnswer:\")]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RAG용 프롬프트 템플릿 작성\n",
    "from langchain import hub\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "example_messages = prompt.invoke(\n",
    "    {\"context\": \"filler context\", \"question\": \"filler question\"}\n",
    ").to_messages()\n",
    "\n",
    "example_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='' id='run-9f016ad6-5479-4485-98b1-f499c14eeba5' usage_metadata={'input_tokens': 983, 'output_tokens': 0, 'total_tokens': 983}content='' id='run-9f016ad6-5479-4485-98b1-f499c14eeba5' usage_metadata={'input_tokens': 983, 'output_tokens': 0, 'total_tokens': 983}\n",
      "content='Task Decomposition' id='run-9f016ad6-5479-4485-98b1-f499c14eeba5'content='Task Decomposition' id='run-9f016ad6-5479-4485-98b1-f499c14eeba5'\n",
      "content=' is the process of' id='run-9f016ad6-5479-4485-98b1-f499c14eeba5'content=' is the process of' id='run-9f016ad6-5479-4485-98b1-f499c14eeba5'\n",
      "content=' breaking down a complex' id='run-9f016ad6-5479-4485-98b1-f499c14eeba5'content=' breaking down a complex' id='run-9f016ad6-5479-4485-98b1-f499c14eeba5'\n",
      "content=' task into smaller,' id='run-9f016ad6-5479-4485-98b1-f499c14eeba5'content=' task into smaller,' id='run-9f016ad6-5479-4485-98b1-f499c14eeba5'\n",
      "content=' more' id='run-9f016ad6-5479-4485-98b1-f499c14eeba5'content=' more' id='run-9f016ad6-5479-4485-98b1-f499c14eeba5'\n",
      "content=' manageable sub' id='run-9f016ad6-5479-4485-98b1-f499c14eeba5'content=' manageable sub' id='run-9f016ad6-5479-4485-98b1-f499c14eeba5'\n",
      "content='-tasks or steps' id='run-9f016ad6-5479-4485-98b1-f499c14eeba5'content='-tasks or steps' id='run-9f016ad6-5479-4485-98b1-f499c14eeba5'\n",
      "content='.' id='run-9f016ad6-5479-4485-98b1-f499c14eeba5'content='.' id='run-9f016ad6-5479-4485-98b1-f499c14eeba5'\n",
      "content=' It is' id='run-9f016ad6-5479-4485-98b1-f499c14eeba5'content=' It is' id='run-9f016ad6-5479-4485-98b1-f499c14eeba5'\n",
      "content=' a key' id='run-9f016ad6-5479-4485-98b1-f499c14eeba5'content=' a key' id='run-9f016ad6-5479-4485-98b1-f499c14eeba5'\n",
      "content=' technique' id='run-9f016ad6-5479-4485-98b1-f499c14eeba5'content=' technique' id='run-9f016ad6-5479-4485-98b1-f499c14eeba5'\n",
      "content=' used to' id='run-9f016ad6-5479-4485-98b1-f499c14eeba5'content=' used to' id='run-9f016ad6-5479-4485-98b1-f499c14eeba5'\n",
      "content=' enhance the performance of' id='run-9f016ad6-5479-4485-98b1-f499c14eeba5'content=' enhance the performance of' id='run-9f016ad6-5479-4485-98b1-f499c14eeba5'\n",
      "content=' large language models on' id='run-9f016ad6-5479-4485-98b1-f499c14eeba5'content=' large language models on' id='run-9f016ad6-5479-4485-98b1-f499c14eeba5'\n",
      "content=' complex tasks by instruct' id='run-9f016ad6-5479-4485-98b1-f499c14eeba5'content=' complex tasks by instruct' id='run-9f016ad6-5479-4485-98b1-f499c14eeba5'\n",
      "content='ing the' id='run-9f016ad6-5479-4485-98b1-f499c14eeba5'content='ing the' id='run-9f016ad6-5479-4485-98b1-f499c14eeba5'\n",
      "content=' model to \"' id='run-9f016ad6-5479-4485-98b1-f499c14eeba5'content=' model to \"' id='run-9f016ad6-5479-4485-98b1-f499c14eeba5'\n",
      "content='think step-' id='run-9f016ad6-5479-4485-98b1-f499c14eeba5'content='think step-' id='run-9f016ad6-5479-4485-98b1-f499c14eeba5'\n",
      "content='by-step\"' id='run-9f016ad6-5479-4485-98b1-f499c14eeba5'content='by-step\"' id='run-9f016ad6-5479-4485-98b1-f499c14eeba5'\n",
      "content=' and decompose the' id='run-9f016ad6-5479-4485-98b1-f499c14eeba5'content=' and decompose the' id='run-9f016ad6-5479-4485-98b1-f499c14eeba5'\n",
      "content=' har' id='run-9f016ad6-5479-4485-98b1-f499c14eeba5'content=' har' id='run-9f016ad6-5479-4485-98b1-f499c14eeba5'\n",
      "content='d task into sim' id='run-9f016ad6-5479-4485-98b1-f499c14eeba5'content='d task into sim' id='run-9f016ad6-5479-4485-98b1-f499c14eeba5'\n",
      "content='pler steps' id='run-9f016ad6-5479-4485-98b1-f499c14eeba5'content='pler steps' id='run-9f016ad6-5479-4485-98b1-f499c14eeba5'\n",
      "content='.' id='run-9f016ad6-5479-4485-98b1-f499c14eeba5'content='.' id='run-9f016ad6-5479-4485-98b1-f499c14eeba5'\n",
      "content=' Methods' id='run-9f016ad6-5479-4485-98b1-f499c14eeba5'content=' Methods' id='run-9f016ad6-5479-4485-98b1-f499c14eeba5'\n",
      "content=' like Chain of' id='run-9f016ad6-5479-4485-98b1-f499c14eeba5'content=' like Chain of' id='run-9f016ad6-5479-4485-98b1-f499c14eeba5'\n",
      "content=' Thought an' id='run-9f016ad6-5479-4485-98b1-f499c14eeba5'content=' Thought an' id='run-9f016ad6-5479-4485-98b1-f499c14eeba5'\n",
      "content='d Tree of Thoughts' id='run-9f016ad6-5479-4485-98b1-f499c14eeba5'content='d Tree of Thoughts' id='run-9f016ad6-5479-4485-98b1-f499c14eeba5'\n",
      "content=' use prompting' id='run-9f016ad6-5479-4485-98b1-f499c14eeba5'content=' use prompting' id='run-9f016ad6-5479-4485-98b1-f499c14eeba5'\n",
      "content=' or' id='run-9f016ad6-5479-4485-98b1-f499c14eeba5'content=' or' id='run-9f016ad6-5479-4485-98b1-f499c14eeba5'\n",
      "content=' task' id='run-9f016ad6-5479-4485-98b1-f499c14eeba5'content=' task' id='run-9f016ad6-5479-4485-98b1-f499c14eeba5'\n",
      "content='-specific instructions to' id='run-9f016ad6-5479-4485-98b1-f499c14eeba5'content='-specific instructions to' id='run-9f016ad6-5479-4485-98b1-f499c14eeba5'\n",
      "content=' guide the model in' id='run-9f016ad6-5479-4485-98b1-f499c14eeba5'content=' guide the model in' id='run-9f016ad6-5479-4485-98b1-f499c14eeba5'\n",
      "content=' decom' id='run-9f016ad6-5479-4485-98b1-f499c14eeba5'content=' decom' id='run-9f016ad6-5479-4485-98b1-f499c14eeba5'\n",
      "content='posing tasks' id='run-9f016ad6-5479-4485-98b1-f499c14eeba5'content='posing tasks' id='run-9f016ad6-5479-4485-98b1-f499c14eeba5'\n",
      "content=' into a' id='run-9f016ad6-5479-4485-98b1-f499c14eeba5'content=' into a' id='run-9f016ad6-5479-4485-98b1-f499c14eeba5'\n",
      "content=' sequence' id='run-9f016ad6-5479-4485-98b1-f499c14eeba5'content=' sequence' id='run-9f016ad6-5479-4485-98b1-f499c14eeba5'\n",
      "content=' or' id='run-9f016ad6-5479-4485-98b1-f499c14eeba5'content=' or' id='run-9f016ad6-5479-4485-98b1-f499c14eeba5'\n",
      "content=' tree of smaller sub' id='run-9f016ad6-5479-4485-98b1-f499c14eeba5'content=' tree of smaller sub' id='run-9f016ad6-5479-4485-98b1-f499c14eeba5'\n",
      "content='-tasks.' id='run-9f016ad6-5479-4485-98b1-f499c14eeba5'content='-tasks.' id='run-9f016ad6-5479-4485-98b1-f499c14eeba5'\n",
      "content='' response_metadata={'stop_reason': 'end_turn', 'stop_sequence': None} id='run-9f016ad6-5479-4485-98b1-f499c14eeba5' usage_metadata={'input_tokens': 0, 'output_tokens': 109, 'total_tokens': 109}content='' response_metadata={'stop_reason': 'end_turn', 'stop_sequence': None} id='run-9f016ad6-5479-4485-98b1-f499c14eeba5' usage_metadata={'input_tokens': 0, 'output_tokens': 109, 'total_tokens': 109}\n"
     ]
    }
   ],
   "source": [
    "# LCEL로 실행 런너블하게 만들기\n",
    "\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\":retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    # | StrOutputParser()\n",
    ")\n",
    "\n",
    "for chunk in rag_chain.stream(\"What is Task Decomposition?\"):\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
